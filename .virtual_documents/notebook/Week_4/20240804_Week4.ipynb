import pandas as pd
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score

df = pd.read_csv("../../dataset/Iris.csv")

df = df.drop('Id', axis=1)
df['Species'] = df['Species'].replace(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'), (0, 1, 2))

features = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
label = df['Species']

clf = SVC(kernel='rbf', gamma=0.1, C=100)

scores = cross_val_score(clf, features, label, cv=10, verbose=3)

print(scores)



from sklearn.datasets import load_wine

wine=load_wine()
x=wine.data
y=wine.target

clf = SVC(kernel='rbf', gamma=0.1, C=100)

scores = cross_val_score(clf, x, y, cv=10, verbose=3)

print(scores)


from sklearn.preprocessing import MinMaxScaler

scaler_minmax = MinMaxScaler()
scaler_minmax.fit(x)
x_train_mmscaled = scaler_minmax.transform(x)
# x_test_mmscaled = scaler_minmax.transform(x_test)

scores = cross_val_score(clf, x_train_mmscaled, y, cv=10, verbose=3)

print(scores)





from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Create a logistic regression classifier
logreg = LogisticRegression(max_iter = 1000)

# Train the classifier
logreg.fit(X_train, y_train)

# Predict the classes for the test set
y_pred = logreg.predict(X_test)

# Calculate the accuracy of the classifier
accuracy = accuracy_score(y_test, y_pred)

# Print the accuracy
print("Accuracy:", accuracy)
print(y_pred)



from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay

cm = confusion_matrix(y_test, y_pred)
cm_display = ConfusionMatrixDisplay(confusion_matrix=cm)
cm_display.plot()
plt.show





from sklearn.model_selection import GridSearchCV

# Define the parameter grid for grid search
param_grid = {
    'C': [0.1, 1.0, 10.0]  # Different inverse regularization strengths
}

# Create a logistic regression classifier
logreg = LogisticRegression(max_iter=1000)

# Perform grid search to find the best hyperparameters
grid_search = GridSearchCV(logreg, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Get the best hyperparameters and the corresponding accuracy score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the logistic regression classifier with the best hyperparameters
best_logreg = LogisticRegression(**best_params, max_iter=1000)
best_logreg.fit(X_train, y_train)

# Predict the classes for the test set
y_pred = best_logreg.predict(X_test)

# Calculate the accuracy of the classifier
accuracy = accuracy_score(y_test, y_pred)

# Print the results
print("Best Hyperparameters:", best_params)
print("Best Accuracy on Cross-Validated Set:", best_score)
print("Accuracy on Test Set:", accuracy)



wine=load_wine()
x=wine.data
y=wine.target
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
param_grid = {
    'C': [0.1, 1.0, 10.0]  # Different inverse regularization strengths
}

# Create a logistic regression classifier
logreg = LogisticRegression(max_iter=10000)

# Perform grid search to find the best hyperparameters
grid_search = GridSearchCV(logreg, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Get the best hyperparameters and the corresponding accuracy score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the logistic regression classifier with the best hyperparameters
best_logreg = LogisticRegression(**best_params, max_iter=10000)
best_logreg.fit(X_train, y_train)

# Predict the classes for the test set
y_pred = best_logreg.predict(X_test)

# Calculate the accuracy of the classifier
accuracy = accuracy_score(y_test, y_pred)

# Print the results
print("Best Hyperparameters:", best_params)
print("Best Accuracy on Cross-Validated Set:", best_score)
print("Accuracy on Test Set:", accuracy)





# Read the dataset
wine_df = pd.read_csv("../../dataset/winequality-red.csv")


wine_df.head()


wine_df.columns


wine_df.quality.nunique()


from collections import Counter
counter = Counter(wine_df.quality)
counter


features = wine_df[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar','chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density','pH', 'sulphates', 'alcohol']]
label = wine_df['quality']


from imblearn.over_sampling import SMOTE

oversample=SMOTE()
features_upsample, label_upsample = oversample.fit_resample(features,label)
counter = Counter(label_upsample)
counter


X_train, X_test, y_train, y_test = train_test_split(features_upsample, 
                                                    label_upsample, 
                                                    test_size=0.1, 
                                                    random_state=123)
param_grid = {
    'C': [0.1, 1.0, 10.0]  # Different inverse regularization strengths
}


scaler_minmax = MinMaxScaler()
scaler_minmax.fit(X_train)
x_train_mmscaled = scaler_minmax.transform(X_train)
x_test_mmscaled = scaler_minmax.transform(X_test)

# Create a logistic regression classifier
logreg = LogisticRegression(max_iter=50000)

# Perform grid search to find the best hyperparameters
grid_search = GridSearchCV(logreg, param_grid, cv=5)
grid_search.fit(x_train_mmscaled, y_train)

# Get the best hyperparameters and the corresponding accuracy score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the logistic regression classifier with the best hyperparameters
best_logreg = LogisticRegression(**best_params, max_iter=50000)
best_logreg.fit(x_train_mmscaled, y_train)

# Predict the classes for the test set
y_pred = best_logreg.predict(x_test_mmscaled)

# Calculate the accuracy of the classifier
accuracy = accuracy_score(y_test, y_pred)

# Print the results
print("Best Hyperparameters:", best_params)
print("Best Accuracy on Cross-Validated Set:", best_score)
print("Accuracy on Test Set:", accuracy)


cm = confusion_matrix(y_test, y_pred)
cm_display = ConfusionMatrixDisplay(confusion_matrix=cm)
cm_display.plot()
plt.show














from sklearn.datasets import fetch_california_housing
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Load the California Housing dataset
data = fetch_california_housing()

X = data.data
y = data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Create a Linear regression model
model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Calculate the mean squared error
mse = mean_squared_error(y_test, y_pred)

# Print the mean squared error
print("Mean Squared Error:", mse)

 








import pandas as pd

house_price = pd.read_csv("../../dataset/housedata/data.csv")


from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
house_price['statezip_encode'] = le.fit_transform(house_price['statezip'])
house_price['city_encode'] = le.fit_transform(house_price['city'])


house_price.head()


house_price_filtered = house_price.drop(['date', 'country', 'city', 'statezip', 'street'], axis=1)
house_price_filtered.dtypes


features=house_price_filtered[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot','floors',
                               'waterfront', 'view', 'condition', 'sqft_above','sqft_basement',
                               'yr_built', 'yr_renovated','statezip_encode', 'city_encode']]

label = house_price_filtered['price']


features


from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score

scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)
x_train, x_test, y_train, y_test = train_test_split(features,
                                                    label,
                                                    test_size=0.1,
                                                    random_state=42)
model = LinearRegression()
model.fit(x_train, y_train)

y_pred = model.predict(x_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(mse)
print(r2)


from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score

scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)

x_train, x_test, y_train, y_test = train_test_split(features, label, test_size=0.1, random_state=42)

param_grid = {
    'fit_intercept': [True, False],
    'positive': [True, False],
    'copy_X': [True, False]
}

model = LinearRegression()

gridsearch = GridSearchCV(model, param_grid, cv=5, verbose=3)

gridsearch.fit(x_train, y_train)

y_pred = gridsearch.predict(x_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# print(mse)
# print(r2)




